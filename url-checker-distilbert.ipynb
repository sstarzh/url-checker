{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic classifier that uses Distilbert model to predict whether supplied URL is malicious\n",
    "##### Distilbert is a smaller and faster version of BERT ( Bidirectional Encoder Representations from Transformers) that is 40% lighter while retaining 97% of BERT's language understanding ability. More on Distilbert at [HuggingFace](https://huggingface.co/docs/transformers/model_doc/distilbert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To begin, let's install and import some packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9o-dl8wLv3Ep"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting ktrain\n",
      "  Using cached ktrain-0.31.2-py3-none-any.whl (25.3 MB)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (3.1.3)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (1.21.6)\n",
      "Collecting langdetect\n",
      "  Using cached langdetect-1.0.9.tar.gz (981 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting whoosh\n",
      "  Using cached Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from ktrain) (0.14.1)\n",
      "Collecting fastprogress>=0.1.21\n",
      "  Using cached fastprogress-1.0.2-py3-none-any.whl (12 kB)\n",
      "Collecting transformers==4.10.3\n",
      "  Using cached transformers-4.10.3-py3-none-any.whl (2.8 MB)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from ktrain) (1.0.1)\n",
      "Collecting cchardet\n",
      "  Downloading cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl (263 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.7/263.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras-bert>=0.86.0\n",
      "  Using cached keras-bert-0.89.0.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting scikit-learn==0.24.2\n",
      "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting syntok==1.3.3\n",
      "  Using cached syntok-1.3.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from ktrain) (20.1)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from ktrain) (2.28.0)\n",
      "Collecting jieba\n",
      "  Using cached jieba-0.42.1.tar.gz (19.2 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: chardet in /opt/conda/lib/python3.7/site-packages (from ktrain) (3.0.4)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.24.2->ktrain) (1.4.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from syntok==1.3.3->ktrain) (2022.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.10.3->ktrain) (4.42.1)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.10.3->ktrain) (3.0.12)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.10.3->ktrain) (1.5.0)\n",
      "Collecting huggingface-hub>=0.0.12\n",
      "  Using cached huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.53.tar.gz (880 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.10.3->ktrain) (6.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.47.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers<2,>=1.12\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.1.0-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.7/123.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m813.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (4.2.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.11.2)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow) (59.3.0)\n",
      "Collecting tensorboard<2.10,>=2.9\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow) (0.34.2)\n",
      "Collecting keras-transformer==0.40.0\n",
      "  Using cached keras-transformer-0.40.0.tar.gz (9.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting keras-pos-embd==0.13.0\n",
      "  Using cached keras-pos-embd-0.13.0.tar.gz (5.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting keras-multi-head==0.29.0\n",
      "  Using cached keras-multi-head-0.29.0.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting keras-layer-normalization==0.16.0\n",
      "  Using cached keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting keras-position-wise-feed-forward==0.8.0\n",
      "  Using cached keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting keras-embed-sim==0.10.0\n",
      "  Using cached keras-embed-sim-0.10.0.tar.gz (3.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting keras-self-attention==0.51.0\n",
      "  Using cached keras-self-attention-0.51.0.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.0.1->ktrain) (2019.3)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.8/97.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.8.0-py2.py3-none-any.whl (164 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.2/164.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.1.2-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->ktrain) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->ktrain) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->ktrain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->ktrain) (2.8)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.8)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting packaging\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting importlib-metadata\n",
      "  Downloading importlib_metadata-4.11.4-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.10.3->ktrain) (2.2.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.10.3->ktrain) (7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.5/151.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: keras-bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention, termcolor, jieba, langdetect, sacremoses\n",
      "  Building wheel for keras-bert (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-bert: filename=keras_bert-0.89.0-py3-none-any.whl size=33517 sha256=4247348959c07f22fbf134f59ca412a3ee324710fd7a7c0dfde1464dc7bbcf09\n",
      "  Stored in directory: /root/.cache/pip/wheels/a4/e8/45/842b3a39831261aef9154b907eacbc4ac99499a99ae829b06f\n",
      "  Building wheel for keras-transformer (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-transformer: filename=keras_transformer-0.40.0-py3-none-any.whl size=12303 sha256=12445ca5f99eb3c9c14602a3e88b4f90a7acd4f7621422a4d5052e1b0dee8cd1\n",
      "  Stored in directory: /root/.cache/pip/wheels/46/68/26/692ed21edd832833c3b0a0e21615bcacd99ca458b3f9ed571f\n",
      "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.10.0-py3-none-any.whl size=3958 sha256=14000d06c82f8af5fa842e83017dec5ab615edfb6b2f507a52e3694b05a4da1d\n",
      "  Stored in directory: /root/.cache/pip/wheels/81/67/b5/d847588d075895281e1cf5590f819bd4cf076a554872268bd5\n",
      "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4668 sha256=f0a6ee12aad0be45310c79762f952147bd4a3fbe944a4d7a0dabe4a2fb835ef4\n",
      "  Stored in directory: /root/.cache/pip/wheels/85/5d/1c/2e619f594f69fbcf8bc20943b27d414871c409be053994813e\n",
      "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14992 sha256=a4c44f6b2907da04c85eb8ebd164e09e4c752f64e9db7c8572e3f219e7145e28\n",
      "  Stored in directory: /root/.cache/pip/wheels/86/aa/3c/9d15d24005179dae08ff291ce99c754b296347817d076fd9fb\n",
      "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.13.0-py3-none-any.whl size=6961 sha256=786599a3419b5575e592a8772610e469557bacd1863e04e1290ba31277cc5fed\n",
      "  Stored in directory: /root/.cache/pip/wheels/8d/c1/a0/dc44fcf68c857b7ff6be9a97e675e5adf51022eff1169b042f\n",
      "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4983 sha256=7f222275985be1e100f502124ad93b982562544d33c66300d16797691dc711ad\n",
      "  Stored in directory: /root/.cache/pip/wheels/c2/75/6f/d42f6e051506f442daeba53ff1e2d21a5f20ef8c411610f2bb\n",
      "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18913 sha256=bba946eaa520c91cfa8a72893c8ffc387889c5d8b4dd3c0855c837842d178c32\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=2f6ac542ef5a48d833658e3b0196d09533b8413ac118d4f8a9ccffee2b2f6b12\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314478 sha256=eee15c72eebebf0b294afe3abbe68b787df15e497b89406eb49c7fd496539307\n",
      "  Stored in directory: /root/.cache/pip/wheels/24/aa/17/5bc7c72e9a37990a9620cc3aad0acad1564dcff6dbc2359de3\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=3063cdbc2d00428f0a7a2a1fa1a7608eb36d34dda9241c3694c49e760a90a8ee\n",
      "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=ed2f55492e10d0b8916d70227f5657f1000fd6de201d4e5824d11777a9e8b00d\n",
      "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
      "Successfully built keras-bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention termcolor jieba langdetect sacremoses\n",
      "Installing collected packages: whoosh, tokenizers, termcolor, tensorboard-plugin-wit, sentencepiece, libclang, keras, jieba, flatbuffers, cchardet, werkzeug, threadpoolctl, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, syntok, sacremoses, pyasn1-modules, protobuf, packaging, opt-einsum, oauthlib, langdetect, keras-self-attention, keras-preprocessing, keras-position-wise-feed-forward, keras-pos-embd, keras-layer-normalization, keras-embed-sim, importlib-metadata, grpcio, gast, fastprogress, cachetools, astunparse, absl-py, scikit-learn, requests-oauthlib, markdown, keras-multi-head, huggingface-hub, google-auth, transformers, keras-transformer, google-auth-oauthlib, tensorboard, keras-bert, tensorflow, ktrain\n",
      "  Attempting uninstall: werkzeug\n",
      "    Found existing installation: Werkzeug 1.0.0\n",
      "    Uninstalling Werkzeug-1.0.0:\n",
      "      Successfully uninstalled Werkzeug-1.0.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.1\n",
      "    Uninstalling protobuf-3.20.1:\n",
      "      Successfully uninstalled protobuf-3.20.1\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 20.1\n",
      "    Uninstalling packaging-20.1:\n",
      "      Successfully uninstalled packaging-20.1\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 1.5.0\n",
      "    Uninstalling importlib-metadata-1.5.0:\n",
      "      Successfully uninstalled importlib-metadata-1.5.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.22.1\n",
      "    Uninstalling scikit-learn-0.22.1:\n",
      "      Successfully uninstalled scikit-learn-0.22.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\n",
      "pytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\n",
      "docker-compose 1.29.2 requires PyYAML<6,>=3.10, but you have pyyaml 6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-1.1.0 astunparse-1.6.3 cachetools-5.2.0 cchardet-2.1.7 fastprogress-1.0.2 flatbuffers-1.12 gast-0.4.0 google-auth-2.8.0 google-auth-oauthlib-0.4.6 grpcio-1.47.0 huggingface-hub-0.8.1 importlib-metadata-4.11.4 jieba-0.42.1 keras-2.9.0 keras-bert-0.89.0 keras-embed-sim-0.10.0 keras-layer-normalization-0.16.0 keras-multi-head-0.29.0 keras-pos-embd-0.13.0 keras-position-wise-feed-forward-0.8.0 keras-preprocessing-1.1.2 keras-self-attention-0.51.0 keras-transformer-0.40.0 ktrain-0.31.2 langdetect-1.0.9 libclang-14.0.1 markdown-3.3.7 oauthlib-3.2.0 opt-einsum-3.3.0 packaging-21.3 protobuf-3.19.4 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 sacremoses-0.0.53 scikit-learn-0.24.2 sentencepiece-0.1.96 syntok-1.3.3 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.26.0 termcolor-1.1.0 threadpoolctl-3.1.0 tokenizers-0.10.3 transformers-4.10.3 werkzeug-2.1.2 whoosh-2.7.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install scikit-learn>=1.0.0\n",
    "!pip3 install ktrain matplotlib tensorflow numpy\n",
    "import matplotlib\n",
    "import os\n",
    "import numpy as np\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some more imports...We are using ktrain wrapper to simplify model operations and take advantage of some cool stuff like simplified data set preprocessing, learning rate finding and \"autofit\" that ensures the model is not overfit. More details on ktrain here: [ktrain on GitHub](https://github.com/amaiya/ktrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "evreWp_bv3Es"
   },
   "outputs": [],
   "source": [
    "import ktrain\n",
    "from ktrain import text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's print the list of available text classifiers in ktrain. There are relatively simple models like fasttext or bigru that have only 7-10 layers, as well as some more sophisticated deep models like BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext: a fastText-like model [http://arxiv.org/pdf/1607.01759.pdf]\n",
      "logreg: logistic regression using a trainable Embedding layer\n",
      "nbsvm: NBSVM model [http://www.aclweb.org/anthology/P12-2018]\n",
      "bigru: Bidirectional GRU with pretrained fasttext word vectors [https://fasttext.cc/docs/en/crawl-vectors.html]\n",
      "standard_gru: simple 2-layer GRU with randomly initialized embeddings\n",
      "bert: Bidirectional Encoder Representations from Transformers (BERT) from keras_bert [https://arxiv.org/abs/1810.04805]\n",
      "distilbert: distilled, smaller, and faster BERT from Hugging Face transformers [https://arxiv.org/abs/1910.01108]\n"
     ]
    }
   ],
   "source": [
    "text.print_text_classifiers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Shmkj8niv3Ev"
   },
   "source": [
    "### Here, we will load our data set. \n",
    "##### We have a CSV file ``trainlist_22k.csv`` that contains a list of HTTP paths that are labeled according to their association with cross-site scripting (xss) and sql injection (sqli).  There is also a \"regular\" traffic that belongs to a \"benign\" class. Data set load is performed using the ```texts_from_csv``` method, which assumes the label_columns are already one-hot-encoded in the spreadsheet. Since *val_filepath* is None, 10% of the data will automatically be used as a validation set.\n",
    "##### In our set we have: 1 feature (payload), 1 label (type) that contains 3 classes:\n",
    " - xss\n",
    " - sqli\n",
    " - benign\n",
    "\n",
    "##### We will be using Distilbert model so preprocessing mode is set to ``Distilbert``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j6-wA79wv3Ew",
    "outputId": "7f7ba49d-d58f-4cf3-f363-806f37d2bd99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected encoding: utf-8 (if wrong, set manually)\n",
      "['bad', 'good']\n",
      "        bad  good\n",
      "291500  0.0   1.0\n",
      "354739  0.0   1.0\n",
      "364516  0.0   1.0\n",
      "217730  0.0   1.0\n",
      "234278  0.0   1.0\n",
      "['bad', 'good']\n",
      "        bad  good\n",
      "22346   1.0   0.0\n",
      "146769  0.0   1.0\n",
      "13610   1.0   0.0\n",
      "259838  0.0   1.0\n",
      "324172  0.0   1.0\n",
      "preprocessing train...\n",
      "language: en\n",
      "train sequence lengths:\n",
      "\tmean : 1\n",
      "\t95percentile : 1\n",
      "\t99percentile : 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 1\n",
      "\t95percentile : 1\n",
      "\t99percentile : 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_PATH = 'urlData.csv'\n",
    "NUM_WORDS = 5000\n",
    "MAXLEN = 200\n",
    "trn, val, preproc = text.texts_from_csv(DATA_PATH,\n",
    "                      'url',\n",
    "                      label_columns = [\"label\"],\n",
    "                      val_filepath=None, # if None, 10% of data will be used for validation\n",
    "                      max_features=NUM_WORDS, maxlen=MAXLEN,\n",
    "                      ngram_range=1,\n",
    "                      preprocess_mode='distilbert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load the learner instance that uses ```Distilbert``` model. We will retain the model structure unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "UpbaZ68Dv3Ew",
    "outputId": "68874f80-6a80-4960-a152-1b594f4ee856"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "maxlen is 200\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "def get_model():\n",
    "    model = text.text_classifier('distilbert', (trn), \n",
    "                             preproc=preproc)\n",
    "    #model.add(Dense(3, activation='sigmoid'))\n",
    "    #model.add(GaussianDropout1D(0.2))\n",
    "    #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "distilbert_model = get_model()\n",
    "#new_model = Sequential()\n",
    "#new_model.add(nbsvm_model)\n",
    "#new_model.add(Dense(2, activation='sigmoid'))\n",
    "#new_model.add(Dropout(0.2))\n",
    "#new_model.add(Dense(2, activation='relu'))\n",
    "distilbert_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "learner = ktrain.get_learner(distilbert_model, train_data=(trn), val_data=(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is what our model looks like. It has a number of layers that are pre-trained therefore allowing us to leverage transfer learning.\n",
    "##### Source code for modeling_tf_distilbert can be found at [HuggingFace Transformers](https://huggingface.co/transformers/v2.3.0/_modules/transformers/modeling_tf_distilbert.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fh1_l7ioIpS6",
    "outputId": "c35e8957-540f-42a7-fd07-d9d417915873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (trainable=True) : <transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertMainLayer object at 0x7f924b87c9d0>\n",
      "1 (trainable=True) : <keras.layers.core.dense.Dense object at 0x7f924b8317d0>\n",
      "2 (trainable=True) : <keras.layers.core.dense.Dense object at 0x7f924b831450>\n",
      "3 (trainable=True) : <keras.layers.regularization.dropout.Dropout object at 0x7f924b831490>\n"
     ]
    }
   ],
   "source": [
    "learner.print_layers()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to ensure that majority of existing pre-trained layers are not re-trained so we are freezing those with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7ATgiIbyE7XJ"
   },
   "outputs": [],
   "source": [
    "learner.freeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The optimal learning rate for this model can be found using the **lr_find** function however it will take at least **20 minutes!** on this VM that uses CPU only. ( Optimal rate was found to be 3e-5 and therefore there is no need to spend time on this now). **If you still want to proceed**, uncomment the command and run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "id": "u-kza8Env3Ex",
    "outputId": "146052d1-5d60-4708-b878-1660db206379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulating training for different learning rates... this may take a few moments...\n",
      "Epoch 1/5\n",
      "11826/11826 [==============================] - 38s 3ms/step - loss: 7.6692 - accuracy: 0.1826\n",
      "Epoch 2/5\n",
      "11826/11826 [==============================] - 37s 3ms/step - loss: 7.6538 - accuracy: 0.1834\n",
      "Epoch 3/5\n",
      "11826/11826 [==============================] - 37s 3ms/step - loss: 2.3351 - accuracy: 0.6713\n",
      "Epoch 4/5\n",
      "11826/11826 [==============================] - 21s 2ms/step - loss: 1.1553 - accuracy: 0.8468\n",
      "\n",
      "\n",
      "done.\n",
      "Visually inspect loss plot and select learning rate associated with falling loss\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.lr_find(show_plot=True, max_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's train the model using the optimal learning rate. More accuracy is achieved after 4-5 epochs however to save time, we will run the cycle using 2 epochs only. That should give us ~93% accuracy and observed loss (binary crossentropy) of ~0.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 3e-05...\n",
      "Epoch 1/2\n",
      "11826/11826 [==============================] - ETA: 0s - loss: 0.4213 - accuracy: 0.8573"
     ]
    }
   ],
   "source": [
    "learner.fit_onecycle(3e-5, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Autofit function can help optimally train the model without ``overfitting`` it. **Do not run** unless you are willing to spend days (or perhaps weeks) on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MmjJwfrQv3Ex",
    "outputId": "17265489-ad02-4108-cb8d-549c06ff93f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early_stopping automatically enabled at patience=5\n",
      "reduce_on_plateau automatically enabled at patience=2\n",
      "\n",
      "\n",
      "begin training using triangular learning rate policy with max lr of 0.01...\n",
      "Epoch 1/1024\n",
      "11826/11826 [==============================] - 45s 4ms/step - loss: 0.5969 - accuracy: 0.8907 - val_loss: 0.1713 - val_accuracy: 0.9548\n",
      "Epoch 2/1024\n",
      "11826/11826 [==============================] - 45s 4ms/step - loss: 0.1974 - accuracy: 0.9518 - val_loss: 0.1618 - val_accuracy: 0.9552\n",
      "Epoch 3/1024\n",
      "11826/11826 [==============================] - 45s 4ms/step - loss: 0.1921 - accuracy: 0.9546 - val_loss: 0.1935 - val_accuracy: 0.9553\n",
      "Epoch 4/1024\n",
      "11826/11826 [==============================] - 45s 4ms/step - loss: 0.1965 - accuracy: 0.9539 - val_loss: 0.1873 - val_accuracy: 0.9560\n",
      "\n",
      "Epoch 00004: Reducing Max LR on Plateau: new max lr will be 0.005 (if not early_stopping).\n",
      "Epoch 5/1024\n",
      "11826/11826 [==============================] - 45s 4ms/step - loss: 0.1762 - accuracy: 0.9560 - val_loss: 0.1850 - val_accuracy: 0.9560\n",
      "Epoch 6/1024\n",
      "11826/11826 [==============================] - 45s 4ms/step - loss: 0.1733 - accuracy: 0.9571 - val_loss: 0.1831 - val_accuracy: 0.9561\n",
      "\n",
      "Epoch 00006: Reducing Max LR on Plateau: new max lr will be 0.0025 (if not early_stopping).\n",
      "Epoch 7/1024\n",
      "11826/11826 [==============================] - 45s 4ms/step - loss: 0.1710 - accuracy: 0.9577 - val_loss: 0.1961 - val_accuracy: 0.9563\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Weights from best epoch have been loaded into model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff576232430>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.autofit(1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alright, let's save our predictor so we can use it to perform inferences outside of the Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor = ktrain.get_predictor(learner.model, preproc)\n",
    "predictor.save('url_checker_distilbert')\n",
    "print('MODEL SAVED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's time for some fun! First, get a predictor instance that uses our pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = ktrain.load_predictor('url_checker_trained_2e')\n",
    "new_model = ktrain.get_predictor(predictor.model, predictor.preproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see if it can catch a malicious URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bad', 0.28830546), ('good', 0.7625915)]\n"
     ]
    }
   ],
   "source": [
    "text = 'yoris-parfums.com'\n",
    "result = new_model.predict(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can run more serious testing outside of the notebook"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ml_sqli_detector.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
